# -*- coding: utf-8 -*-
"""ReinforcedLearningKP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NmHgBO2ZjWXKJP9luRpL83DDTXpV78b2
"""

# Load libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import math
import random
import sqlite3
import keras
from collections import deque
from keras.models import Sequential
from keras.models import load_model
from keras.layers import Dense
from keras.optimizers import Adam
import tensorflow as tf

# 데이터베이스에서 가격 정보를 가져오고, None 또는 NaN 값이 있는 행을 삭제하는 함수
def get_symbol_data(symbol, timestamp):
    conn = sqlite3.connect('symbols.db')
    cursor = conn.cursor()

    # 종목별 테이블에서 데이터 조회
    cursor.execute(f"SELECT timestamp, kimchi_premium_usdt, bybit_close_krw_usdt, upbit_close FROM {symbol} WHERE timestamp = ?", (timestamp,))
    data = cursor.fetchone()

    conn.close()

    if data:
        # None이나 NaN 값을 가진 데이터를 확인하고 처리
        kimchi_premium, bybit_price, upbit_price, timestamp = data
        if any(x is None or pd.isna(x) for x in [kimchi_premium, bybit_price, upbit_price, timestamp]):
            return None  # None이거나 NaN인 값이 있으면 데이터를 반환하지 않음
        return kimchi_premium, bybit_price, upbit_price, timestamp
    else:
        return None

# 데이터베이스에서 가격 데이터를 로드하면서 NaN 값을 가진 행을 삭제하는 함수
def clean_symbol_data(symbol):
    conn = sqlite3.connect('symbols.db')
    query = f"SELECT * FROM {symbol}"
    df = pd.read_sql(query, conn)

    # None이나 NaN 값을 포함한 행을 삭제
    df_cleaned = df.dropna(subset=['kimchi_premium_usdt', 'bybit_close_krw_usdt', 'upbit_close','timestamp'])

    # 클린 데이터를 다시 데이터베이스에 저장
    df_cleaned.to_sql(symbol, conn, if_exists='replace', index=False)
    conn.close()

# 예시: AAVE 테이블에서 가격 데이터를 클린징
clean_symbol_data('AAVE')

import sqlite3
import numpy as np

# 데이터베이스 연결
conn = sqlite3.connect('symbols.db')
cursor = conn.cursor()

# AAVE 테이블에서 데이터 가져오기 (timestamp, upbit_close, bybit_close_krw_usdt, kimchi_premium_usdt)
cursor.execute("SELECT * FROM AAVE")
data = cursor.fetchall()

# 데이터가 (timestamp, upbit_close, bybit_close_krw_usdt, kimchi_premium_usdt) 형식이라면, 필요한 데이터 추출
upbit_prices = [row[1] for row in data]  # Upbit 가격 (두 번째 열)
bybit_prices = [row[4] for row in data]  # Bybit 가격 (세 번째 열)
kimchi_premium = [row[6] for row in data]  # Kimchi Premium (네 번째 열)

# 데이터셋을 numpy 배열로 변환
dataset1 = np.array(list(zip(upbit_prices, bybit_prices, kimchi_premium)))



# 연결 종료
conn.close()

# 첫 번째, 네 번째, 여섯 번째 열 선택
columns_to_select = [0, 1, 2]  # 첫 번째, 네 번째, 여섯 번째 열의 인덱스

# NumPy 배열을 Pandas DataFrame으로 변환
dataset = pd.DataFrame(dataset1, columns=["col1", "col2", "col3"])

# 첫 번째, 네 번째, 여섯 번째 열 선택
selected_data = dataset[["col3"]]

# 2D 데이터를 1D로 변환
X = selected_data.values.flatten()  # NumPy 배열로 변환 후 펼치기
X = [float(x) for x in X]  # float로 변환

# print("Selected Data:", X)
dataset=dataset.fillna(method='ffill')
# print('Null Values =',dataset.isnull().values.any())

selected_data = dataset.iloc[:, [2]]

# 데이터를 리스트로 변환
X = selected_data.values.flatten()  # 2D 배열을 1D로 변환
X = [float(x) for x in X]  # float로 변환

validation_size = 0.2
#In case the data is not dependent on the time series, then train and test split should be done based on sequential sample
#This can be done by selecting an arbitrary split point in the ordered list of observations and creating two new datasets.
train_size = int(len(X) * (1-validation_size))
X_train, X_test = X[0:train_size], X[train_size:len(X)]
# X_train의 상위 3개 값 출력
# print("X_train 상위 3개 값:", X_train[:6])
# dataset.iloc[:, [2]].plot()

# from tensorflow.keras.models import Sequential
# from tensorflow.keras.models import load_model
# from tensorflow.keras.layers import Dense
# from tensorflow.keras.optimizers import Adam

class Agent:
    def __init__(self, state_size, is_eval=False, model_name=""):
        #State size depends and is equal to the the window size, n previous days
        self.state_size = state_size # normalized previous days,
        self.action_size = 3 # sit, buy, sell
        self.memory = deque(maxlen=1000)
        self.inventory = []
        self.model_name = model_name
        self.is_eval = is_eval

        self.gamma = 0.95
        self.epsilon = 1.0
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995
        #self.epsilon_decay = 0.9

        #self.model = self._model()

        self.model = tf.keras.models.load_model(model_name) if is_eval else self._model()

    #Deep Q Learning model- returns the q-value when given state as input
    def _model(self):
        model = tf.keras.models.Sequential()
        #Input Layer
        model.add(tf.keras.layers.Dense(units=64, input_dim=self.state_size, activation="relu"))
        #Hidden Layers
        model.add(tf.keras.layers.Dense(units=32, activation="relu"))
        model.add(tf.keras.layers.Dense(units=8, activation="relu"))
        #Output Layer
        model.add(tf.keras.layers.Dense(self.action_size, activation="linear"))
        model.compile(loss="mse", optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))
        return model

    #Return the action on the value function
    #With probability (1-$\epsilon$) choose the action which has the highest Q-value.
    #With probability ($\epsilon$) choose any action at random.
    #Intitially high epsilon-more random, later less
    #The trained agents were evaluated by different initial random condition
    #and an e-greedy policy with epsilon 0.05. This procedure is adopted to minimize the possibility of overfitting during evaluation.

    def act(self, state):
        #If it is test and self.epsilon is still very high, once the epsilon become low, there are no random
        #actions suggested.
        if not self.is_eval and random.random() <= self.epsilon:
            return random.randrange(self.action_size)
        options = self.model.predict(state)
        #set_trace()
        #action is based on the action that has the highest value from the q-value function.
        return np.argmax(options[0])

    def expReplay(self, batch_size):
        mini_batch = []
        l = len(self.memory)
        for i in range(l - batch_size + 1, l):
            mini_batch.append(self.memory[i])

        # the memory during the training phase.
        for state, action, reward, next_state, done in mini_batch:
            target = reward # reward or Q at time t
            #update the Q table based on Q table equation
            #set_trace()
            if not done:
                #set_trace()
                #max of the array of the predicted.
                target = reward + self.gamma * np.amax(self.model.predict(next_state)[0])

            # Q-value of the state currently from the table
            target_f = self.model.predict(state)
            # Update the output Q table for the given action in the table
            target_f[0][action] = target
            #train and fit the model where state is X and target_f is Y, where the target is updated.
            self.model.fit(state, target_f, epochs=1, verbose=0)

        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

# prints formatted price
def formatPrice(n):
    return ("-$" if n < 0 else "$") + "{0:.2f}".format(abs(n))

# # returns the vector containing stock data from a fixed file
# def getStockData(key):
#     vec = []
#     lines = open("data/" + key + ".csv", "r").read().splitlines()

#     for line in lines[1:]:
#         vec.append(float(line.split(",")[4])) #Only Close column

#     return vec

# returns the sigmoid
def sigmoid(x):
    return 1 / (1 + math.exp(-x))

# returns an an n-day state representation ending at time t

def getState(data, t, n):
    d = t - n + 1
    block = data[d:t + 1] if d >= 0 else -d * [data[0]] + data[0:t + 1] # pad with t0
    #block is which is the for [1283.27002, 1283.27002]
    res = []
    for i in range(n - 1):
        res.append(sigmoid(block[i + 1] - block[i]))
    return np.array([res])


# Plots the behavior of the output
def plot_behavior(data_input, states_buy, states_sell, profit):
    fig = plt.figure(figsize = (15,5))
    plt.plot(data_input, color='r', lw=2.)
    plt.plot(data_input, '^', markersize=10, color='m', label = 'Buying signal', markevery = states_buy)
    plt.plot(data_input, 'v', markersize=10, color='k', label = 'Selling signal', markevery = states_sell)
    plt.title('Total gains: %f'%(profit))
    plt.legend()
    #plt.savefig('output/'+name+'.png')
    plt.show()

# Agent 객체 생성
window_size = 1
agent = Agent(window_size)

# 학습 데이터 (X_train)
data = X_train
l = len(data) - 1

batch_size = 32
episode_count = 10

# 에피소드 시작
for e in range(episode_count + 1):
    print("Running episode " + str(e) + "/" + str(episode_count))
    state = getState(data, 0, window_size + 1)
    total_profit = 0
    agent.inventory = []
    states_sell = []
    states_buy = []

    # 데이터 순차적 처리
    for t in range(l):
        action = agent.act(state)
        next_state = getState(data, t + 1, window_size + 1)
        reward = 0

        if action == 1:  # 매수 (Upbit에서 매수, Bybit에서 공매도)
            upbit_buy_price = dataset.iloc[t, 0]  # col1
            bybit_sell_price = dataset.iloc[t, 1]  # col2)

            # Upbit에서 매수 및 Bybit에서 공매도 동시에 진행
            agent.inventory.append(upbit_buy_price)
            agent.inventory.append(bybit_sell_price)  # 공매도 시 가격 저장
            states_buy.append(t)
            # print("Buy: Upbit " + formatPrice(upbit_buy_price) + " | Bybit Short: " + formatPrice(bybit_sell_price))

        elif action == 2 and len(agent.inventory) > 0:  # 매도 (Upbit에서 매도, Bybit에서 청산)
            bought_upbit_price = agent.inventory.pop(0)  # 매수한 Upbit 가격
            bought_bybit_price = agent.inventory.pop(0)  # 공매도한 Bybit 가격

            # 현재 시점에서의 가격
            upbit_close = dataset.iloc[t, 0]  # 현재 시점의 Upbit 가격
            bybit_close_krw_usdt = dataset.iloc[t, 0]  # 현재 시점의 Bybit 가격

            # 수수료 계산 (예시로 수수료 0.05%로 가정)
            upbit_fee = 0.0005
            bybit_fee = 0.00055

            # Upbit 매도 시 수수료
            upbit_sell_fee = upbit_close * upbit_fee
            # Bybit 청산 시 수수료
            bybit_close_fee = bybit_close_krw_usdt * bybit_fee

            # Upbit에서 매도 및 Bybit에서 청산 후의 손익 계산
            upbit_profit = upbit_close - bought_upbit_price - upbit_sell_fee  # Upbit 손익
            bybit_profit = bought_bybit_price - bybit_close_krw_usdt - bybit_close_fee  # Bybit 손익 (공매도 청산)

            # 총 손익 계산
            reward = upbit_profit + bybit_profit
            total_profit += reward

            states_sell.append(t)
            # print("Sell: Upbit " + formatPrice(upbit_close) + " | Bybit Close: " + formatPrice(bybit_close_krw_usdt) + " | Profit: " + formatPrice(reward))


        done = True if t == l - 1 else False

        # 에이전트 메모리에 상태와 보상 저장
        agent.memory.append((state, action, reward, next_state, done))
        state = next_state

        if done:
            print("--------------------------------")
            print("Total Profit: " + formatPrice(total_profit))
            print("--------------------------------")
            # 행동 결과 시각화
            plot_behavior(data, states_buy, states_sell, total_profit)

        # 메모리가 충분히 쌓이면 경험 재플레이
        if len(agent.memory) > batch_size:
            agent.expReplay(batch_size)

    # 일정 에피소드마다 모델 저장
    if e % 2 == 0:
        agent.model.save(working_dir + "model_ep" + str(e))
